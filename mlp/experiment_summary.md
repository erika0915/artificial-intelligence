#  MNIST MLP ì‹¤í—˜ ì •ë¦¬

> ëª©í‘œ: ë‹¤ì–‘í•œ MLP ëª¨ë¸ êµ¬ì¡°, í™œì„±í™” í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, í•™ìŠµë¥ ì„ ì¡°í•©í•˜ì—¬ MNIST ìˆ«ì ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë¹„êµí•˜ê³  í•™ìŠµ ê³¡ì„ ì„ ì‹œê°í™”í•¨.

## ğŸ“Œ ì‹¤í—˜ ê°œìš”
- ë°ì´í„°ì…‹: MNIST (28Ã—28 í‘ë°± ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€)
- í´ë˜ìŠ¤ ìˆ˜: 10ê°œ (0~9)
- ëª¨ë¸ êµ¬ì¡°: MLP (Fully-connected ì‹ ê²½ë§)
- ëª©í‘œ: ì´ë¯¸ì§€ â†’ ìˆ«ì ë¶„ë¥˜

## ğŸ§ª ì‹¤í—˜
### Experiment 1: ê¸°ë³¸ MLP

- êµ¬ì¡°: 2-layer MLP (784 â†’ 32 â†’ 10)
- Activation: Sigmoid
- Optimizer: SGD (lr=0.1)
- Test Accuracy: 95.03%
- Test Loss: 0.1444
- í•™ìŠµ ê³¡ì„ :  
  ![image](https://github.com/user-attachments/assets/3f01fd5b-b7a5-4bc6-9295-3c40524defa9)


### Experiment 2: Deep MLP (ì€ë‹‰ì¸µ 6ê°œ)

- êµ¬ì¡°: 6-layer MLP (784 â†’ 32 x 6 â†’ 10)
- Activation: Sigmoid
- Optimizer: SGD (lr=0.1)
- Test Accuracy: 11.35%
- Test Loss: 2.3019
- Overfitting + Gradient Vanishing í˜„ìƒ ë°œìƒ
- í•™ìŠµ ê³¡ì„ :  
  ![image](https://github.com/user-attachments/assets/e4e69cda-492c-47ca-ac64-d2c1dbfcc9cc)



### Experiment 3: Wide MLP (512 units)

- êµ¬ì¡°: 2-layer MLP (784 â†’ 512 â†’ 10)
- Activation: Sigmoid
- Optimizer: SGD (lr=0.1)
- Test Accuracy: 97.08%
- Test Loss: 0.0966
- í•™ìŠµ ê³¡ì„ :  
  ![image](https://github.com/user-attachments/assets/ff0523c9-61ca-48d3-ad6f-ff4c4d43a54d)



### Experiment 4: Activation Function ë¹„êµ

- êµ¬ì¡°: 2-layer MLP (784 â†’ 32 â†’ 10)
- Optimizer: SGD (lr=0.1)
- ë¹„êµ í•­ëª©:
  - Sigmoid â†’ Accuracy: ë‚®ìŒ
  - Tanh â†’ ì¤‘ê°„
  - ReLU â†’ ê°€ì¥ ìš°ìˆ˜
- Best Accuracy: 96.84% (ReLU ê¸°ì¤€)
- Test Loss: 0.1127
- í•™ìŠµ ê³¡ì„ :  
  ![image](https://github.com/user-attachments/assets/0593367b-6932-4ed4-a269-283de55cc539)



### Experiment 5: Optimizer ë³€ê²½ (Adam)

- êµ¬ì¡°: 2-layer MLP (784 â†’ 512 â†’ 10)
- Activation: Sigmoid
- Optimizer: Adam (lr=0.1)
- Test Accuracy: 83.94%
- Test Loss: 0.7173
- í•™ìŠµ ë¶ˆì•ˆì •: ê³¼í•œ learning rateë¡œ ì¸í•œ ì§„ë™
- í•™ìŠµ ê³¡ì„ :  
  ![image](https://github.com/user-attachments/assets/32d597e1-c5e0-412d-ae38-ce7b5c76e423)



### Experiment 6: Learning Rate ë¹„êµ (Adam)

- êµ¬ì¡°: 2-layer MLP (784 â†’ 512 â†’ 10)
- Activation: Sigmoid
- Optimizer: Adam
- í•™ìŠµë¥  ë¹„êµ:
  - lr = 1.0: Accuracy 11.35%
  - lr = 0.1: Accuracy 96.99%
  - lr = 0.01: Accuracy 97.19% (ìµœê³  ì„±ëŠ¥)
- í•™ìŠµ ê³¡ì„ :  
  ![image](https://github.com/user-attachments/assets/aa6a2b28-2bc0-4a50-8d01-fa2bb2d20f29)



## ğŸ“Œ ê²°ë¡  ìš”ì•½

| ì‹¤í—˜ | êµ¬ì¡°                  | íŠ¹ì´ì  / ë³€ê²½ì               | Accuracy                          | ë¹„ê³                                |
|------|-----------------------|-------------------------------|-----------------------------------|------------------------------------|
| 1    | ê¸°ë³¸ MLP              | -                             | 95.03%                            | ì•ˆì •ì  ê¸°ë³¸ ì„±ëŠ¥                   |
| 2    | ê¹Šì€ MLP (6ì¸µ)        | ì€ë‹‰ì¸µ 6ê°œ                    | 11.35%                            | í•™ìŠµ ì‹¤íŒ¨ (vanishing)              |
| 3    | ë„“ì€ MLP (512 units)  | ì€ë‹‰ ìœ ë‹› 512                 | 97.08%                            | ì„±ëŠ¥ í–¥ìƒ                          |
| 4    | ë‹¤ì–‘í•œ Activation     | Sigmoid / Tanh / ReLU ë¹„êµ   | 96.84% (ReLU ê¸°ì¤€)                | ReLU ì„±ëŠ¥ ê°€ì¥ ìš°ìˆ˜                |
| 5    | Optimizer ë³€ê²½        | Adam, lr=0.1                  | 83.94%                            | ë¶ˆì•ˆì • (lr ë„ˆë¬´ í¼)                |
| 6    | í•™ìŠµë¥  ë¹„êµ (Adam)    | lr=1.0 / 0.1 / 0.01           | 97.19% (lr=0.01 ê¸°ì¤€ ìµœê³  ì„±ëŠ¥)   | lr=0.01ì´ ê°€ì¥ ì•ˆì •                |


